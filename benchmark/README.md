# Automatic benchmark generation suite

Suite to generate benchmarks across different WaterLily versions, Julia versions (using [**juliaup**](https://github.com/JuliaLang/juliaup)), backends, cases, and cases sizes using the [benchmark.sh](./benchmark.sh) script.

## TL;DR
Usage example
```sh
sh benchmark.sh -v "1.9.4 1.10.0" -t "1 4" -b "Array CuArray" -c "tgv jelly" -p "6,7 5,6" -s "100 100" -ft "Float32 Float64"
julia --project compare.jl --datadir="data" --plotdir="plots" --patterns=["tgv","jelly"] --sort=1
```
runs both the TGV and jelly benchmarks (`-c`) at the current WaterLily state in 2 different Julia versions (1.9.4 and 1.10.0-rc1, noting that these need to be available in juliaup), and 3 different backends (CPUx01, CPUx04, CUDA). The cases size `-p`, number of time steps `-s`, and float type `-ft` are bash (ordered) arrays which need to be equally sized to `-c` and specify each benchmark case (respectively).
The default benchmarks launch (`sh benchmark.sh`) is equivalent to:
```sh
sh benchmark.sh -w "" -ju true -v release -t "1 4" -b "Array CuArray" -c "tgv jelly" -p "6,7 5,6" -s "100 100" -ft "Float32 Float32"
```
Note that `-w` or `--waterlily` can be used to pass different WaterLily versions, but this feature will only work for WaterLily>1.2 releases (since checking out to an old WaterLily version will also change the benchmark suite). This will be fixed by moving the benchmark suite to its own repository.

Benchmarks are then post-processed using the `compare.jl` script. Plots can be generated by passing the `--plotdir` argument. Note that `--patterns` can be passed to post-process only certain benchmarks. Alternatively, benchmark files can also be passed directly as arguments with
```sh
julia --project compare.jl --plotdir="plots" $(find data/ \( -name "tgv*json" -o -name "jelly*json" \) -printf "%T@ %Tc %p\n" | sort -n | awk '{print $7}')
```

## Usage information
The accepted command line arguments are (parenthesis for short version):
 - Backend arguments: `--waterlily(-w)`, `--juliaup(-ju)`, `--versions(-v)`, `--backends(-b)`, `--threads(-t)`. Respectively: List of WaterLily git hashes to test, usage of juliaup or not (`true` by default, but use `false` to run with default Julia in system), Julia version, backend types, number of threads (when `--backends` contains `Array`). The latter 3 arguments accept a list of different parameters, for example:
    ```sh
    -w "fae590d e22ad41" -v "1.8.5 1.9.4" -b "Array CuArray" -t "1 6"
    ```
    would generate benchmark for all these combinations of parameters.
 - Case arguments: `--cases(-c)`, `--log2p(-p)`, `--max_steps(-s)`, `--ftype(-ft)`. The `--cases` argument specifies which cases to benchmark, and it can be again a list of different cases. The name of the cases needs to be defined in [benchmark.jl](./benchmark.jl), for example `tgv` or `jelly`. The current available cases are `"tgv sphere cylinder donut jelly"`. Hence, to add a new case first define the function that returns a `Simulation` in [benchmark.jl](./benchmark.jl), and then it can be called using the `--cases(-c)` list argument. Case size, number of time steps, and float data type are then defined for each case (`-p`, `-s`, `-ft`, respectively). All case arguments must have an equal length since each element of the array defines the case in different aspects.

The following command
```sh
sh benchmark.sh -v release -t "1 3 6" -b "Array CuArray" -c "tgv sphere" -p "6,7,8 5,6" -s "10 100" -ft "Float64 Float32"
```
would allow running benchmarks with 4 backends: CPUx01 (serial), CPUx03, CPUx06, GPU. Additionally, two benchmarks would be tested, `tgv` and `sphere`, with different sizes, number of time steps, and float type, each. This would result into 1 Julia version x (3 Array + 1 CuArray) backends x (3 TGV sizes + 2 jelly sizes) = 20 benchmarks.

Benchmarks are saved in JSON format with the following nomenclature: `casename_sizes_maxsteps_ftype_backend_waterlilyHEADhash_juliaversion.json`. Benchmarks can be finally compared using [`compare.jl`](./compare.jl) as follows
```sh
julia --project compare.jl benchmark_1.json benchmark_2.json benchmark_3.json ...
```
or by using pattern syntax
```sh
julia --project compare.jl --datadir="data" --patterns=["tgv*CPU"]
```
for which only TGV benchmarks on a CPU backend found in the `"data"` directory would be processed. The following syntax would be produce equivalent results:
```sh
julia --project compare.jl $(find data -name "tgv*CPU.json" -printf "%T@ %Tc %p\n" | sort -n | awk '{print $7}') --sort=1
```
by taking the `tgv` JSON files, sort them by creation time, and pass them as arguments to the `compare.jl` program. Finally, note that the first benchmark passed as argument is taken as reference to compute speed-ups of other benchmarks: `speedup_x = time(benchmark_1) / time(benchmark_x)`. The `--sort=<1 to 8>` argument can also be used when running the comparison. It will sort the benchmark table rows by the values corresponding to the column index passed as argument. `--sort=1` corresponds to sorting by backend. The baseline row is highlighted in blue, and the fastest run in a table is highlighted in green.
